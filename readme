üöÄ CDC Event Processor - Complete Implementation
High-Performance Event Processing met Debezium + Kafka + Python
üìã Project Overzicht
Architectuur: PostgreSQL WAL ‚Üí Debezium ‚Üí Kafka ‚Üí Python Async Consumer ‚Üí Handler Registry ‚Üí MongoDB
Latency: <100ms end-to-end
Scalability: Horizontaal schaalbaar (meerdere consumers)
Flexibility: Pluggable event handlers per event_type

üìÅ Complete Project Structuur
event-processor/
‚îú‚îÄ‚îÄ docker-compose.yml                 # Complete orchestration
‚îú‚îÄ‚îÄ .env                               # Environment variables
‚îú‚îÄ‚îÄ debezium/
‚îÇ   ‚îî‚îÄ‚îÄ postgres-connector.json        # Debezium PostgreSQL connector config
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                        # Application entry point
‚îÇ   ‚îú‚îÄ‚îÄ config.py                      # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ consumer.py                    # Kafka consumer met error handling
‚îÇ   ‚îú‚îÄ‚îÄ registry.py                    # Handler registry pattern
‚îÇ   ‚îú‚îÄ‚îÄ models.py                      # Pydantic event models
‚îÇ   ‚îú‚îÄ‚îÄ handlers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py                    # Base handler class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_handlers.py           # User event handlers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ activity_handlers.py       # Activity event handlers
‚îÇ   ‚îî‚îÄ‚îÄ database/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ mongodb.py                 # MongoDB connection manager
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_handlers.py
‚îÇ   ‚îî‚îÄ‚îÄ test_consumer.py
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ README.md

üìÑ FILE: docker-compose.yml
Path: event-processor/docker-compose.yml
Type: YAML
yamlversion: '3.8'

services:
  # ===========================
  # KAFKA ECOSYSTEM
  # ===========================
  
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - event-processor-network
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data
      - zookeeper-logs:/var/lib/zookeeper/log

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    networks:
      - event-processor-network
    volumes:
      - kafka-data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 5

  # ===========================
  # DEBEZIUM CDC
  # ===========================
  
  debezium:
    image: debezium/connect:2.4
    container_name: debezium
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      GROUP_ID: debezium-cluster
      CONFIG_STORAGE_TOPIC: debezium_configs
      OFFSET_STORAGE_TOPIC: debezium_offsets
      STATUS_STORAGE_TOPIC: debezium_statuses
      # Replication factors for single-node setup
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
    networks:
      - event-processor-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8083/"]
      interval: 10s
      timeout: 10s
      retries: 5

  # ===========================
  # PYTHON EVENT PROCESSOR
  # ===========================
  
  event-processor:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: event-processor
    depends_on:
      kafka:
        condition: service_healthy
      debezium:
        condition: service_healthy
    environment:
      # Kafka
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC: postgres.activity.event_outbox
      KAFKA_GROUP_ID: event-processor-group
      KAFKA_AUTO_OFFSET_RESET: earliest
      
      # MongoDB
      MONGODB_URI: ${MONGODB_URI}
      MONGODB_DATABASE: ${MONGODB_DATABASE:-activity_read}
      
      # PostgreSQL (voor status updates als nodig)
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT:-5432}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      
      # Application
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      PROCESSING_BATCH_SIZE: ${PROCESSING_BATCH_SIZE:-100}
      MAX_RETRIES: ${MAX_RETRIES:-3}
      
    networks:
      - event-processor-network
    restart: unless-stopped
    volumes:
      - ./app:/app
      - ./logs:/logs

  # ===========================
  # KAFKA UI (Optional - voor monitoring)
  # ===========================
  
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - event-processor-network

networks:
  event-processor-network:
    driver: bridge

volumes:
  zookeeper-data:
  zookeeper-logs:
  kafka-data:

üìÑ FILE: .env
Path: event-processor/.env
Type: Environment Variables
bash# PostgreSQL Configuration
POSTGRES_HOST=your-postgres-host
POSTGRES_PORT=5432
POSTGRES_DB=activity
POSTGRES_USER=your-user
POSTGRES_PASSWORD=your-password

# MongoDB Configuration
MONGODB_URI=mongodb://your-mongo-host:27017
MONGODB_DATABASE=activity_read

# Application Configuration
LOG_LEVEL=INFO
PROCESSING_BATCH_SIZE=100
MAX_RETRIES=3

# Kafka Configuration (defaults in docker-compose)
# KAFKA_BOOTSTRAP_SERVERS=localhost:9092
# KAFKA_TOPIC=postgres.activity.event_outbox

üìÑ FILE: debezium/postgres-connector.json
Path: event-processor/debezium/postgres-connector.json
Type: JSON (Debezium Connector Configuration)
json{
  "name": "postgres-event-outbox-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "YOUR_POSTGRES_HOST",
    "database.port": "5432",
    "database.user": "YOUR_POSTGRES_USER",
    "database.password": "YOUR_POSTGRES_PASSWORD",
    "database.dbname": "activity",
    "database.server.name": "postgres",
    "table.include.list": "activity.event_outbox",
    "plugin.name": "pgoutput",
    "publication.name": "debezium_publication",
    "slot.name": "debezium_slot",
    
    "transforms": "route",
    "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
    "transforms.route.regex": "([^.]+)\\.([^.]+)\\.([^.]+)",
    "transforms.route.replacement": "$1.$2.$3",
    
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter.schemas.enable": "false",
    
    "tombstones.on.delete": "false",
    "snapshot.mode": "never",
    "heartbeat.interval.ms": "5000",
    
    "topic.prefix": "postgres",
    "decimal.handling.mode": "double",
    "time.precision.mode": "adaptive_time_microseconds",
    "include.schema.changes": "false"
  }
}

üìÑ FILE: requirements.txt
Path: event-processor/requirements.txt
Type: Python Dependencies
txt# Async Kafka Client
aiokafka==0.10.0

# MongoDB Async Driver
motor==3.3.2
pymongo==4.6.1

# PostgreSQL Driver (voor optionele status updates)
asyncpg==0.29.0

# Data Validation
pydantic==2.5.0
pydantic-settings==2.1.0

# Logging & Monitoring
structlog==23.2.0
python-json-logger==2.0.7

# Utilities
python-dotenv==1.0.0
tenacity==8.2.3  # Retry logic

# Testing
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-cov==4.1.0

# Development
black==23.12.0
ruff==0.1.8
mypy==1.7.1

üìÑ FILE: Dockerfile
Path: event-processor/Dockerfile
Type: Dockerfile
dockerfileFROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first (for layer caching)
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY app/ ./app/

# Create logs directory
RUN mkdir -p /logs

# Run as non-root user
RUN useradd -m -u 1000 processor && chown -R processor:processor /app /logs
USER processor

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD python -c "import sys; sys.exit(0)"

# Run the event processor
CMD ["python", "-m", "app.main"]

üìÑ FILE: app/init.py
Path: event-processor/app/__init__.py
Type: Python
python"""
Event Processor Application
High-performance CDC-based event processing for PostgreSQL ‚Üí MongoDB
"""

__version__ = "1.0.0"

üìÑ FILE: app/config.py
Path: event-processor/app/config.py
Type: Python
python"""
Configuration Management
Centralized settings using Pydantic for validation
"""

from pydantic_settings import BaseSettings
from typing import Optional


class Settings(BaseSettings):
    """Application Configuration"""
    
    # Kafka Settings
    kafka_bootstrap_servers: str = "localhost:9092"
    kafka_topic: str = "postgres.activity.event_outbox"
    kafka_group_id: str = "event-processor-group"
    kafka_auto_offset_reset: str = "earliest"
    kafka_enable_auto_commit: bool = False
    kafka_max_poll_records: int = 100
    
    # MongoDB Settings
    mongodb_uri: str
    mongodb_database: str = "activity_read"
    mongodb_connect_timeout_ms: int = 5000
    mongodb_server_selection_timeout_ms: int = 5000
    
    # PostgreSQL Settings (optional - voor status updates)
    postgres_host: Optional[str] = None
    postgres_port: int = 5432
    postgres_db: Optional[str] = None
    postgres_user: Optional[str] = None
    postgres_password: Optional[str] = None
    
    # Application Settings
    log_level: str = "INFO"
    processing_batch_size: int = 100
    max_retries: int = 3
    retry_delay_seconds: int = 5
    shutdown_timeout_seconds: int = 30
    
    class Config:
        env_file = ".env"
        case_sensitive = False


# Global settings instance
settings = Settings()

üìÑ FILE: app/models.py
Path: event-processor/app/models.py
Type: Python
python"""
Event Models
Pydantic models voor event validation en type safety
"""

from pydantic import BaseModel, Field, UUID4
from typing import Dict, Any, Optional
from datetime import datetime
from enum import Enum


class EventStatus(str, Enum):
    """Event processing status"""
    PENDING = "pending"
    PROCESSING = "processing"
    PROCESSED = "processed"
    FAILED = "failed"


class OutboxEvent(BaseModel):
    """
    Event Outbox Model
    Represents a single event from the event_outbox table
    """
    event_id: UUID4
    sequence_id: int
    aggregate_id: UUID4
    aggregate_type: str
    event_type: str
    payload: Dict[str, Any]
    status: EventStatus
    retry_count: int = 0
    last_error: Optional[str] = None
    lock_id: Optional[UUID4] = None
    created_at: datetime
    published_at: Optional[datetime] = None
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat(),
            UUID4: lambda v: str(v)
        }


class DebeziumPayload(BaseModel):
    """
    Debezium CDC Message Format
    The structure that Debezium sends to Kafka
    """
    op: str  # 'c' = create, 'u' = update, 'd' = delete, 'r' = read (snapshot)
    ts_ms: int  # Timestamp
    before: Optional[Dict[str, Any]] = None
    after: Dict[str, Any]
    source: Dict[str, Any]
    
    def to_outbox_event(self) -> OutboxEvent:
        """Convert Debezium payload to OutboxEvent"""
        return OutboxEvent(**self.after)


class ProcessingResult(BaseModel):
    """Result of event processing"""
    success: bool
    event_id: UUID4
    event_type: str
    handler_name: str
    error: Optional[str] = None
    processing_time_ms: float

üìÑ FILE: app/database/mongodb.py
Path: event-processor/app/database/mongodb.py
Type: Python
python"""
MongoDB Connection Manager
Async MongoDB client met connection pooling
"""

import asyncio
from motor.motor_asyncio import AsyncIOMotorClient
from typing import Optional
import structlog

from app.config import settings

logger = structlog.get_logger()


class MongoDBManager:
    """
    MongoDB Connection Manager
    Singleton pattern voor hergebruikbare database connectie
    """
    
    _instance: Optional['MongoDBManager'] = None
    _client: Optional[AsyncIOMotorClient] = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    async def connect(self):
        """Establish MongoDB connection"""
        if self._client is None:
            try:
                self._client = AsyncIOMotorClient(
                    settings.mongodb_uri,
                    connectTimeoutMS=settings.mongodb_connect_timeout_ms,
                    serverSelectionTimeoutMS=settings.mongodb_server_selection_timeout_ms,
                )
                # Test connection
                await self._client.admin.command('ping')
                logger.info(
                    "mongodb_connected",
                    uri=settings.mongodb_uri.split('@')[-1],  # Hide credentials
                    database=settings.mongodb_database
                )
            except Exception as e:
                logger.error("mongodb_connection_failed", error=str(e))
                raise
    
    async def disconnect(self):
        """Close MongoDB connection"""
        if self._client:
            self._client.close()
            self._client = None
            logger.info("mongodb_disconnected")
    
    @property
    def client(self) -> AsyncIOMotorClient:
        """Get MongoDB client"""
        if self._client is None:
            raise RuntimeError("MongoDB not connected. Call connect() first.")
        return self._client
    
    @property
    def db(self):
        """Get database instance"""
        return self.client[settings.mongodb_database]
    
    def collection(self, name: str):
        """Get collection by name"""
        return self.db[name]


# Global MongoDB manager instance
mongodb = MongoDBManager()

üìÑ FILE: app/handlers/base.py
Path: event-processor/app/handlers/base.py
Type: Python
python"""
Base Event Handler
Abstract base class voor alle event handlers
"""

from abc import ABC, abstractmethod
from typing import Dict, Any
import structlog

from app.models import OutboxEvent
from app.database.mongodb import mongodb

logger = structlog.get_logger()


class BaseEventHandler(ABC):
    """
    Base Event Handler
    
    Elke handler implementeert deze interface.
    Dit zorgt voor consistentie en makkelijke uitbreiding.
    """
    
    @property
    @abstractmethod
    def event_type(self) -> str:
        """Het event type dat deze handler afhandelt"""
        pass
    
    @property
    def handler_name(self) -> str:
        """Naam van de handler (voor logging)"""
        return self.__class__.__name__
    
    @abstractmethod
    async def handle(self, event: OutboxEvent) -> None:
        """
        Process the event
        
        Args:
            event: The outbox event to process
            
        Raises:
            Exception: Als processing faalt
        """
        pass
    
    async def validate(self, event: OutboxEvent) -> bool:
        """
        Optional validation logic
        
        Override in subclass als je specifieke validatie wilt
        """
        return True
    
    def log_event(self, event: OutboxEvent, message: str, **kwargs):
        """Helper voor structured logging"""
        logger.info(
            message,
            handler=self.handler_name,
            event_type=event.event_type,
            event_id=str(event.event_id),
            aggregate_id=str(event.aggregate_id),
            **kwargs
        )
    
    def log_error(self, event: OutboxEvent, error: Exception, **kwargs):
        """Helper voor error logging"""
        logger.error(
            "handler_error",
            handler=self.handler_name,
            event_type=event.event_type,
            event_id=str(event.event_id),
            error=str(error),
            error_type=type(error).__name__,
            **kwargs
        )

üìÑ FILE: app/handlers/user_handlers.py
Path: event-processor/app/handlers/user_handlers.py
Type: Python
python"""
User Event Handlers
Handlers voor user-gerelateerde events
"""

from typing import Dict, Any
from datetime import datetime

from app.handlers.base import BaseEventHandler
from app.models import OutboxEvent
from app.database.mongodb import mongodb


class UserCreatedHandler(BaseEventHandler):
    """
    Handler voor UserCreated events
    
    Schrijft het user document naar de 'users' collectie in MongoDB
    """
    
    @property
    def event_type(self) -> str:
        return "UserCreated"
    
    async def handle(self, event: OutboxEvent) -> None:
        """
        Create user document in MongoDB
        
        Event payload structure:
        {
            "user_id": "uuid",
            "email": "user@example.com",
            "username": "johndoe",
            "first_name": "John",
            "last_name": "Doe",
            "created_at": "2024-01-01T12:00:00Z"
        }
        """
        self.log_event(event, "processing_user_created")
        
        payload = event.payload
        
        # Build MongoDB document
        user_doc = {
            "_id": str(event.aggregate_id),  # Use aggregate_id as _id
            "email": payload.get("email"),
            "username": payload.get("username"),
            "name": f"{payload.get('first_name', '')} {payload.get('last_name', '')}".strip(),
            "first_name": payload.get("first_name"),
            "last_name": payload.get("last_name"),
            "profile": {
                "bio": payload.get("bio"),
                "avatar_url": payload.get("avatar_url"),
            },
            "metadata": {
                "created_at": event.created_at,
                "updated_at": datetime.utcnow(),
                "source_event_id": str(event.event_id),
            },
            # Voor IDOR prevention
            "allowed_users": [str(event.aggregate_id)],
        }
        
        # Insert into MongoDB
        users_collection = mongodb.collection("users")
        await users_collection.insert_one(user_doc)
        
        self.log_event(
            event,
            "user_created_success",
            user_id=str(event.aggregate_id),
            username=payload.get("username")
        )


class UserUpdatedHandler(BaseEventHandler):
    """
    Handler voor UserUpdated events
    
    Update user document in MongoDB
    """
    
    @property
    def event_type(self) -> str:
        return "UserUpdated"
    
    async def handle(self, event: OutboxEvent) -> None:
        """Update user document"""
        self.log_event(event, "processing_user_updated")
        
        payload = event.payload
        user_id = str(event.aggregate_id)
        
        # Build update document
        update_fields = {}
        
        if "email" in payload:
            update_fields["email"] = payload["email"]
        if "username" in payload:
            update_fields["username"] = payload["username"]
        if "first_name" in payload or "last_name" in payload:
            first_name = payload.get("first_name", "")
            last_name = payload.get("last_name", "")
            update_fields["name"] = f"{first_name} {last_name}".strip()
            update_fields["first_name"] = first_name
            update_fields["last_name"] = last_name
        
        if "bio" in payload:
            update_fields["profile.bio"] = payload["bio"]
        if "avatar_url" in payload:
            update_fields["profile.avatar_url"] = payload["avatar_url"]
        
        # Always update metadata
        update_fields["metadata.updated_at"] = datetime.utcnow()
        update_fields["metadata.last_event_id"] = str(event.event_id)
        
        # Update MongoDB
        users_collection = mongodb.collection("users")
        result = await users_collection.update_one(
            {"_id": user_id},
            {"$set": update_fields}
        )
        
        if result.matched_count == 0:
            self.log_error(event, Exception(f"User not found: {user_id}"))
            raise ValueError(f"User not found: {user_id}")
        
        self.log_event(
            event,
            "user_updated_success",
            user_id=user_id,
            modified_count=result.modified_count
        )


class UserStatisticsHandler(BaseEventHandler):
    """
    Handler voor UserCreated events
    
    Update global statistics (voorbeeld van multiple handlers voor √©√©n event type)
    """
    
    @property
    def event_type(self) -> str:
        return "UserCreated"
    
    async def handle(self, event: OutboxEvent) -> None:
        """Increment global user count"""
        self.log_event(event, "updating_user_statistics")
        
        stats_collection = mongodb.collection("statistics")
        
        await stats_collection.update_one(
            {"_id": "global_stats"},
            {
                "$inc": {"total_users": 1},
                "$set": {"last_updated": datetime.utcnow()}
            },
            upsert=True
        )
        
        self.log_event(event, "user_statistics_updated")

üìÑ FILE: app/handlers/activity_handlers.py
Path: event-processor/app/handlers/activity_handlers.py
Type: Python
python"""
Activity Event Handlers
Handlers voor activity-gerelateerde events
"""

from typing import Dict, Any
from datetime import datetime

from app.handlers.base import BaseEventHandler
from app.models import OutboxEvent
from app.database.mongodb import mongodb


class ActivityCreatedHandler(BaseEventHandler):
    """
    Handler voor ActivityCreated events
    
    Schrijft activity document naar MongoDB
    """
    
    @property
    def event_type(self) -> str:
        return "ActivityCreated"
    
    async def handle(self, event: OutboxEvent) -> None:
        """Create activity document"""
        self.log_event(event, "processing_activity_created")
        
        payload = event.payload
        
        # Build activity document
        activity_doc = {
            "_id": str(event.aggregate_id),
            "title": payload.get("title"),
            "description": payload.get("description"),
            "creator_id": payload.get("creator_user_id"),
            "type": payload.get("activity_type"),
            "location": {
                "name": payload.get("location_name"),
                "address": payload.get("location_address"),
                "coordinates": payload.get("coordinates"),  # GeoJSON
            },
            "schedule": {
                "start_date": payload.get("start_date"),
                "end_date": payload.get("end_date"),
                "timezone": payload.get("timezone"),
            },
            "participants": {
                "current_count": 0,
                "max_count": payload.get("max_participants"),
                "list": [],
            },
            "status": "active",
            "metadata": {
                "created_at": event.created_at,
                "updated_at": datetime.utcnow(),
                "source_event_id": str(event.event_id),
            },
            # IDOR prevention - wie mag dit zien?
            "allowed_users": [payload.get("creator_user_id")],
        }
        
        # Insert into MongoDB
        activities_collection = mongodb.collection("activities")
        await activities_collection.insert_one(activity_doc)
        
        self.log_event(
            event,
            "activity_created_success",
            activity_id=str(event.aggregate_id),
            title=payload.get("title")
        )


class ParticipantJoinedHandler(BaseEventHandler):
    """
    Handler voor ParticipantJoined events
    
    Update activity document met nieuwe participant
    """
    
    @property
    def event_type(self) -> str:
        return "ParticipantJoined"
    
    async def handle(self, event: OutboxEvent) -> None:
        """Add participant to activity"""
        self.log_event(event, "processing_participant_joined")
        
        payload = event.payload
        activity_id = str(event.aggregate_id)
        user_id = payload.get("user_id")
        
        # Update activity document
        activities_collection = mongodb.collection("activities")
        
        result = await activities_collection.update_one(
            {"_id": activity_id},
            {
                "$addToSet": {
                    "participants.list": {
                        "user_id": user_id,
                        "joined_at": event.created_at,
                        "status": "confirmed",
                    },
                    "allowed_users": user_id,  # IDOR: participant kan nu deze activity zien
                },
                "$inc": {"participants.current_count": 1},
                "$set": {
                    "metadata.updated_at": datetime.utcnow(),
                    "metadata.last_event_id": str(event.event_id),
                }
            }
        )
        
        if result.matched_count == 0:
            self.log_error(event, Exception(f"Activity not found: {activity_id}"))
            raise ValueError(f"Activity not found: {activity_id}")
        
        self.log_event(
            event,
            "participant_joined_success",
            activity_id=activity_id,
            user_id=user_id
        )


class ActivityUpdatedHandler(BaseEventHandler):
    """
    Handler voor ActivityUpdated events
    """
    
    @property
    def event_type(self) -> str:
        return "ActivityUpdated"
    
    async def handle(self, event: OutboxEvent) -> None:
        """Update activity document"""
        self.log_event(event, "processing_activity_updated")
        
        payload = event.payload
        activity_id = str(event.aggregate_id)
        
        # Build update document
        update_fields = {}
        
        if "title" in payload:
            update_fields["title"] = payload["title"]
        if "description" in payload:
            update_fields["description"] = payload["description"]
        if "status" in payload:
            update_fields["status"] = payload["status"]
        
        # Nested updates
        if "location_name" in payload:
            update_fields["location.name"] = payload["location_name"]
        if "location_address" in payload:
            update_fields["location.address"] = payload["location_address"]
        
        # Metadata
        update_fields["metadata.updated_at"] = datetime.utcnow()
        update_fields["metadata.last_event_id"] = str(event.event_id)
        
        # Update MongoDB
        activities_collection = mongodb.collection("activities")
        result = await activities_collection.update_one(
            {"_id": activity_id},
            {"$set": update_fields}
        )
        
        if result.matched_count == 0:
            raise ValueError(f"Activity not found: {activity_id}")
        
        self.log_event(
            event,
            "activity_updated_success",
            activity_id=activity_id
        )

üìÑ FILE: app/handlers/init.py
Path: event-processor/app/handlers/__init__.py
Type: Python
python"""
Event Handlers Module
Export alle handlers voor gemakkelijke import
"""

from app.handlers.user_handlers import (
    UserCreatedHandler,
    UserUpdatedHandler,
    UserStatisticsHandler,
)
from app.handlers.activity_handlers import (
    ActivityCreatedHandler,
    ActivityUpdatedHandler,
    ParticipantJoinedHandler,
)

__all__ = [
    "UserCreatedHandler",
    "UserUpdatedHandler",
    "UserStatisticsHandler",
    "ActivityCreatedHandler",
    "ActivityUpdatedHandler",
    "ParticipantJoinedHandler",
]

üìÑ FILE: app/registry.py
Path: event-processor/app/registry.py
Type: Python
python"""
Handler Registry
Centralized registration en lookup van event handlers
"""

from typing import Dict, List, Type
import structlog

from app.handlers.base import BaseEventHandler
from app.handlers import (
    UserCreatedHandler,
    UserUpdatedHandler,
    UserStatisticsHandler,
    ActivityCreatedHandler,
    ActivityUpdatedHandler,
    ParticipantJoinedHandler,
)

logger = structlog.get_logger()


class HandlerRegistry:
    """
    Handler Registry
    
    Beheert alle event handlers en routeert events naar de juiste handlers.
    
    Key Design: Meerdere handlers kunnen luisteren naar hetzelfde event_type!
    """
    
    def __init__(self):
        self._handlers: Dict[str, List[BaseEventHandler]] = {}
        self._initialize_handlers()
    
    def _initialize_handlers(self):
        """
        Register all handlers
        
        HIER VOEG JE NIEUWE HANDLERS TOE!
        Dit is de enige plek waar je wijzigingen maakt bij nieuwe event types.
        """
        
        # User handlers
        self.register(UserCreatedHandler())
        self.register(UserStatisticsHandler())  # Luistert ook naar UserCreated!
        self.register(UserUpdatedHandler())
        
        # Activity handlers
        self.register(ActivityCreatedHandler())
        self.register(ActivityUpdatedHandler())
        self.register(ParticipantJoinedHandler())
        
        # Log registered handlers
        for event_type, handlers in self._handlers.items():
            handler_names = [h.handler_name for h in handlers]
            logger.info(
                "handlers_registered",
                event_type=event_type,
                handlers=handler_names,
                count=len(handlers)
            )
    
    def register(self, handler: BaseEventHandler):
        """
        Register een handler voor zijn event_type
        
        Args:
            handler: Een instantie van BaseEventHandler
        """
        event_type = handler.event_type
        
        if event_type not in self._handlers:
            self._handlers[event_type] = []
        
        self._handlers[event_type].append(handler)
        
        logger.debug(
            "handler_registered",
            event_type=event_type,
            handler=handler.handler_name
        )
    
    def get_handlers(self, event_type: str) -> List[BaseEventHandler]:
        """
        Get alle handlers voor een event_type
        
        Args:
            event_type: Het event type (bijv. "UserCreated")
            
        Returns:
            List van handlers (kan leeg zijn)
        """
        return self._handlers.get(event_type, [])
    
    def has_handlers(self, event_type: str) -> bool:
        """Check of er handlers zijn voor een event_type"""
        return event_type in self._handlers and len(self._handlers[event_type]) > 0
    
    @property
    def registered_event_types(self) -> List[str]:
        """Get alle geregistreerde event types"""
        return list(self._handlers.keys())


# Global registry instance
handler_registry = HandlerRegistry()

üìÑ FILE: app/consumer.py
Path: event-processor/app/consumer.py
Type: Python
python"""
Kafka Consumer
High-performance async Kafka consumer met error handling
"""

import asyncio
import json
from typing import Optional, Dict, Any
from contextlib import asynccontextmanager
import time

from aiokafka import AIOKafkaConsumer
from aiokafka.errors import KafkaError
import structlog

from app.config import settings
from app.models import DebeziumPayload, OutboxEvent, ProcessingResult
from app.registry import handler_registry
from app.database.mongodb import mongodb

logger = structlog.get_logger()


class EventConsumer:
    """
    Kafka Event Consumer
    
    Consumeert events van Kafka topic en routeert ze naar handlers
    """
    
    def __init__(self):
        self.consumer: Optional[AIOKafkaConsumer] = None
        self.running = False
        self._processing_count = 0
        self._error_count = 0
        self._start_time = time.time()
    
    async def initialize(self):
        """Initialize Kafka consumer"""
        try:
            self.consumer = AIOKafkaConsumer(
                settings.kafka_topic,
                bootstrap_servers=settings.kafka_bootstrap_servers,
                group_id=settings.kafka_group_id,
                auto_offset_reset=settings.kafka_auto_offset_reset,
                enable_auto_commit=settings.kafka_enable_auto_commit,
                max_poll_records=settings.kafka_max_poll_records,
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            )
            
            await self.consumer.start()
            
            logger.info(
                "kafka_consumer_started",
                topic=settings.kafka_topic,
                group_id=settings.kafka_group_id,
                bootstrap_servers=settings.kafka_bootstrap_servers
            )
            
        except Exception as e:
            logger.error("kafka_consumer_init_failed", error=str(e))
            raise
    
    async def shutdown(self):
        """Graceful shutdown"""
        logger.info("kafka_consumer_shutting_down")
        self.running = False
        
        if self.consumer:
            await self.consumer.stop()
        
        # Log statistics
        uptime = time.time() - self._start_time
        logger.info(
            "kafka_consumer_stopped",
            total_processed=self._processing_count,
            total_errors=self._error_count,
            uptime_seconds=round(uptime, 2)
        )
    
    async def process_message(self, message) -> Optional[ProcessingResult]:
        """
        Process a single Kafka message
        
        Args:
            message: Kafka ConsumerRecord
            
        Returns:
            ProcessingResult or None if skipped
        """
        start_time = time.time()
        
        try:
            # Parse Debezium CDC message
            debezium_payload = DebeziumPayload(**message.value)
            
            # Skip deletes and snapshots
            if debezium_payload.op in ('d', 'r'):
                logger.debug(
                    "message_skipped",
                    op=debezium_payload.op,
                    partition=message.partition,
                    offset=message.offset
                )
                return None
            
            # Convert to OutboxEvent
            event = debezium_payload.to_outbox_event()
            
            # Get handlers for this event type
            handlers = handler_registry.get_handlers(event.event_type)
            
            if not handlers:
                logger.warning(
                    "no_handlers_found",
                    event_type=event.event_type,
                    event_id=str(event.event_id)
                )
                return None
            
            # Execute all handlers for this event
            logger.info(
                "processing_event",
                event_type=event.event_type,
                event_id=str(event.event_id),
                handler_count=len(handlers)
            )
            
            for handler in handlers:
                try:
                    # Validate event (if handler implements validation)
                    if not await handler.validate(event):
                        logger.warning(
                            "event_validation_failed",
                            handler=handler.handler_name,
                            event_type=event.event_type,
                            event_id=str(event.event_id)
                        )
                        continue
                    
                    # Process event
                    await handler.handle(event)
                    
                except Exception as handler_error:
                    logger.error(
                        "handler_failed",
                        handler=handler.handler_name,
                        event_type=event.event_type,
                        event_id=str(event.event_id),
                        error=str(handler_error),
                        error_type=type(handler_error).__name__
                    )
                    # Continue met volgende handler (niet stoppen!)
                    self._error_count += 1
            
            # Processing complete
            processing_time = (time.time() - start_time) * 1000  # milliseconds
            self._processing_count += 1
            
            logger.info(
                "event_processed",
                event_type=event.event_type,
                event_id=str(event.event_id),
                processing_time_ms=round(processing_time, 2),
                total_processed=self._processing_count
            )
            
            return ProcessingResult(
                success=True,
                event_id=event.event_id,
                event_type=event.event_type,
                handler_name="multiple",
                processing_time_ms=processing_time
            )
            
        except Exception as e:
            logger.error(
                "message_processing_failed",
                error=str(e),
                error_type=type(e).__name__,
                partition=message.partition,
                offset=message.offset
            )
            self._error_count += 1
            return None
    
    async def consume(self):
        """
        Main consumption loop
        
        Consumeert berichten van Kafka en verwerkt ze asynchroon
        """
        self.running = True
        
        logger.info("starting_event_consumption")
        
        try:
            async for message in self.consumer:
                if not self.running:
                    logger.info("consumption_stopped_by_flag")
                    break
                
                # Process message
                await self.process_message(message)
                
                # Commit offset (manual commit)
                if not settings.kafka_enable_auto_commit:
                    await self.consumer.commit()
                
        except asyncio.CancelledError:
            logger.info("consumption_cancelled")
        except Exception as e:
            logger.error("consumption_error", error=str(e), error_type=type(e).__name__)
            raise
        finally:
            await self.shutdown()
    
    @property
    def stats(self) -> Dict[str, Any]:
        """Get consumer statistics"""
        return {
            "running": self.running,
            "total_processed": self._processing_count,
            "total_errors": self._error_count,
            "uptime_seconds": round(time.time() - self._start_time, 2),
        }

üìÑ FILE: app/main.py
Path: event-processor/app/main.py
Type: Python
python"""
Main Application Entry Point
Orchestrates de event processing service
"""

import asyncio
import signal
import sys
from contextlib import asynccontextmanager

import structlog

from app.config import settings
from app.consumer import EventConsumer
from app.database.mongodb import mongodb
from app.registry import handler_registry

# Configure structured logging
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.add_log_level,
        structlog.processors.JSONRenderer()
    ]
)

logger = structlog.get_logger()


class Application:
    """
    Main Application
    
    Beheert de lifecycle van de event processor
    """
    
    def __init__(self):
        self.consumer = EventConsumer()
        self.shutdown_event = asyncio.Event()
    
    def setup_signal_handlers(self):
        """Setup graceful shutdown handlers"""
        
        def signal_handler(sig, frame):
            logger.info("shutdown_signal_received", signal=sig)
            self.shutdown_event.set()
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    
    async def startup(self):
        """Application startup"""
        logger.info(
            "application_starting",
            version="1.0.0",
            kafka_topic=settings.kafka_topic,
            mongodb_database=settings.mongodb_database
        )
        
        # Connect to MongoDB
        await mongodb.connect()
        
        # Initialize Kafka consumer
        await self.consumer.initialize()
        
        # Log registered handlers
        logger.info(
            "handlers_ready",
            event_types=handler_registry.registered_event_types
        )
        
        logger.info("application_started")
    
    async def shutdown(self):
        """Application shutdown"""
        logger.info("application_shutting_down")
        
        # Stop consumer
        await self.consumer.shutdown()
        
        # Disconnect MongoDB
        await mongodb.disconnect()
        
        logger.info("application_stopped")
    
    async def run(self):
        """Main application loop"""
        try:
            # Startup
            await self.startup()
            
            # Start consuming events
            consume_task = asyncio.create_task(self.consumer.consume())
            
            # Wait for shutdown signal
            await self.shutdown_event.wait()
            
            # Cancel consumption
            consume_task.cancel()
            try:
                await consume_task
            except asyncio.CancelledError:
                pass
            
        except Exception as e:
            logger.error("application_error", error=str(e), error_type=type(e).__name__)
            raise
        finally:
            await self.shutdown()


async def main():
    """Entry point"""
    app = Application()
    app.setup_signal_handlers()
    
    try:
        await app.run()
    except KeyboardInterrupt:
        logger.info("keyboard_interrupt")
    except Exception as e:
        logger.error("fatal_error", error=str(e))
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())

üìÑ FILE: README.md
Path: event-processor/README.md
Type: Markdown
markdown# üöÄ Event Processor - CDC Architecture

High-performance event processing system using Debezium CDC, Kafka, and Python.

## üèóÔ∏è Architecture
PostgreSQL WAL ‚Üí Debezium ‚Üí Kafka ‚Üí Python Consumer ‚Üí Handler Registry ‚Üí MongoDB
‚Üì
[UserCreatedHandler]
[ActivityCreatedHandler]
[ParticipantJoinedHandler]
[... custom handlers]

## ‚ö° Performance

- **Latency**: <100ms end-to-end (PostgreSQL commit ‚Üí MongoDB write)
- **Throughput**: 1000+ events/second per consumer
- **Scalability**: Horizontaal schaalbaar (multiple consumers)

## üöÄ Quick Start

### 1. Prerequisites

- Docker & Docker Compose
- PostgreSQL met event_outbox tabel
- MongoDB instance

### 2. Configuration

Create `.env` file:
```bash
cp .env.example .env
# Edit .env met jouw database credentials
```

### 3. Start Services
```bash
# Start alle services
docker-compose up -d

# Check logs
docker-compose logs -f event-processor
```

### 4. Configure Debezium Connector
```bash
# Wait for Debezium to be ready
docker-compose logs -f debezium

# Deploy PostgreSQL connector
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d @debezium/postgres-connector.json
```

### 5. Verify
```bash
# Check Kafka topics
docker-compose exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Check Kafka UI (optional)
open http://localhost:8080
```

## üîß Development

### Add New Event Handler

1. Create handler in `app/handlers/`:
```python
from app.handlers.base import BaseEventHandler
from app.models import OutboxEvent

class MyEventHandler(BaseEventHandler):
    @property
    def event_type(self) -> str:
        return "MyEventType"
    
    async def handle(self, event: OutboxEvent) -> None:
        # Your processing logic
        pass
```

2. Register in `app/registry.py`:
```python
from app.handlers.my_handler import MyEventHandler

def _initialize_handlers(self):
    # ... existing handlers
    self.register(MyEventHandler())
```

3. Restart service:
```bash
docker-compose restart event-processor
```

## üìä Monitoring

### View Logs
```bash
# Application logs
docker-compose logs -f event-processor

# Kafka logs
docker-compose logs -f kafka

# Debezium logs
docker-compose logs -f debezium
```

### Check Consumer Stats

Logs include structured metrics:
- `total_processed`: Total events processed
- `total_errors`: Total processing errors
- `processing_time_ms`: Per-event processing time

## üß™ Testing
```bash
# Run tests
docker-compose exec event-processor pytest

# With coverage
docker-compose exec event-processor pytest --cov=app
```

## üêõ Troubleshooting

### Consumer not receiving events?

1. Check Debezium connector status:
```bash
curl http://localhost:8083/connectors/postgres-event-outbox-connector/status
```

2. Check Kafka topic:
```bash
docker-compose exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic postgres.activity.event_outbox \
  --from-beginning
```

3. Check PostgreSQL replication:
```sql
SELECT * FROM pg_replication_slots;
SELECT * FROM pg_publication;
```

### High latency?

- Check network between services
- Verify Kafka partition count
- Scale consumers horizontally
- Optimize handler performance

## üìö Documentation

- [Debezium PostgreSQL Connector](https://debezium.io/documentation/reference/stable/connectors/postgresql.html)
- [Kafka Documentation](https://kafka.apache.org/documentation/)
- [Motor (MongoDB async)](https://motor.readthedocs.io/)

## ü§ù Contributing

1. Create new handler
2. Add tests
3. Update registry
4. Submit PR

## üìù License

MIT

üéØ DEPLOYMENT INSTRUCTIES
Stap 1: PostgreSQL Setup (WAL Configuration)
Voer deze SQL uit op je PostgreSQL database:
sql-- Enable logical replication
ALTER SYSTEM SET wal_level = 'logical';

-- Restart PostgreSQL (vereist)
-- sudo systemctl restart postgresql

-- Create publication voor Debezium
CREATE PUBLICATION debezium_publication FOR TABLE activity.event_outbox;

-- Verify
SELECT * FROM pg_publication;
Stap 2: Build & Deploy
bash# Clone/copy project
cd event-processor

# Configure environment
cp .env.example .env
nano .env  # Fill in your credentials

# Start services
docker-compose up -d

# Watch logs
docker-compose logs -f
Stap 3: Deploy Debezium Connector
bash# Wait for Debezium to be ready (check logs)
docker-compose logs debezium | grep "Kafka Connect started"

# Update postgres-connector.json met jouw credentials
nano debezium/postgres-connector.json

# Deploy connector
curl -X POST http://localhost:8083/connectors \
  -H "Content-Type: application/json" \
  -d @debezium/postgres-connector.json

# Verify connector
curl http://localhost:8083/connectors/postgres-event-outbox-connector/status
Stap 4: Test Event Flow
sql-- Insert test event in PostgreSQL
INSERT INTO activity.event_outbox (
    event_id,
    aggregate_id,
    aggregate_type,
    event_type,
    payload,
    status
) VALUES (
    uuidv7(),
    uuidv7(),
    'User',
    'UserCreated',
    '{"email": "test@example.com", "username": "testuser", "first_name": "Test", "last_name": "User"}'::jsonb,
    'pending'
);

-- Check event processor logs
-- docker-compose logs -f event-processor

-- Verify in MongoDB
-- db.users.findOne({email: "test@example.com"})

üé® Key Design Patterns
1. Handler Registry Pattern
Meerdere handlers kunnen luisteren naar hetzelfde event type:

UserCreated ‚Üí UserCreatedHandler + UserStatisticsHandler

2. Flexible Event Processing
Nieuwe handlers toevoegen = 2 stappen:

Create handler class
Register in registry

3. Error Isolation
Als √©√©n handler faalt, gaan andere handlers gewoon door.
4. Horizontal Scalability
Run meerdere consumers:
bashdocker-compose up --scale event-processor=3
Kafka's partitioning zorgt voor load balancing.

üìà Performance Optimizations
Configured Defaults:

Batch size: 100 events per poll
Auto-commit: Disabled (manual commit voor at-least-once)
Heartbeat interval: 5 seconds
MongoDB connection pool: Async motor client

Tuning Options in .env:
bashPROCESSING_BATCH_SIZE=100       # Higher = meer throughput, meer latency
KAFKA_MAX_POLL_RECORDS=100     # Kafka fetch batch size

üîê Security Considerations

Network Isolation: Services in dedicated Docker network
Credentials: Never hardcode, use .env
MongoDB: Use connection string with auth
PostgreSQL: Create dedicated replication user
Kafka: Consider TLS in production


üéì Learning Resources
Understanding CDC

What is Change Data Capture?

Kafka Concepts

Kafka in 5 minutes

Handler Pattern

Chain of Responsibility


‚úÖ Production Checklist

 PostgreSQL WAL level = 'logical'
 Debezium connector deployed & running
 MongoDB indexes created
 .env file configured
 Docker containers healthy
 Test event flow verified
 Monitoring/alerting setup
 Backup strategy voor Kafka offsets


Gemaakt met ‚ù§Ô∏è voor high-performance event processing
